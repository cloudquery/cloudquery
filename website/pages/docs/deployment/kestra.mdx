---
title: Orchestrating CloudQuery Syncs with Kestra
description: Learn how to run CloudQuery as a Kestra flow, using the HackerNews API as a source and CSV file as a destination.
tag: tutorial
date: 2024/01/22
---

# Orchestrating CloudQuery Syncs with Kestra

This tutorial shows how to run CloudQuery as a [Kestra](https://kestra.io/) flow, using the HackerNews API as a source and CSV file as a destination. Kestra is an open-source event-driven orchestrator, which has a [dedicated CloudQuery plugin](https://kestra.io/plugins/plugin-cloudquery).

You can use Kestra to:

- schedule [CloudQuery syncs](https://kestra.io/plugins/tasks/io.kestra.plugin.cloudquery.Sync) in a simple, declarative way using a built-in code editor
- run your syncs based on events from other systems e.g. a new file in your S3 bucket, new message in a queue, or a new webhook request
- monitor your syncs from a friendly UI and get notified when they fail.

## Prerequisites

- [Docker installed](https://docs.docker.com/get-docker/)
- [A CloudQuery API key](/docs/deployment/generate-api-key)

## Step 1: Install Kestra

Follow the [Getting started guide](https://kestra.io/docs/getting-started) to launch Kestra locally in Docker. It's as easy as running a single Docker command:

```bash
docker run --pull=always --rm -it -p 8080:8080 --user=root -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/tmp kestra/kestra:latest server local
```

Once the container is running, open http://localhost:8080 in your browser.

## Step 2: Create a Kestra flow

Inside the Kestra UI, go to the `Flows` page and click on `Create`. You can now paste the following content (make sure to update the connection string and set the CloudQuery API key):

```yaml
id: hn_to_csv
namespace: dev
description: Hackernews to CSV

tasks:
  - id: wdir
    type: io.kestra.core.tasks.flows.WorkingDirectory
    tasks:
      - id: config_files
        type: io.kestra.core.tasks.storages.LocalFiles
        inputs:
          config.yml: |
            kind: source
            spec:
              name: hackernews
              path: cloudquery/hackernews
              registry: cloudquery
              version: v3.0.21
              tables: ["*"]
              destinations:
                - file
              spec:
                item_concurrency: 100
                start_time: "2024-04-30T00:00:00Z"
            ---
            kind: destination
            spec:
              name: "file"
              path: "cloudquery/file"
              registry: "cloudquery"
              version: "v4.1.2"
              write_mode: "append"
              spec:
                path: "/tmp/sync/{{'{{TABLE}}/{{UUID}}.{{FORMAT}}'}}"
                format: "csv" # options: parquet, json, csv
      - id: run_sync
        type: io.kestra.plugin.cloudquery.CloudQueryCLI
        env:
          # Visit https://kestra.io/docs/concepts/secret to learn how to manage secrets in Kestra
          CLOUDQUERY_API_KEY: "{{ secret('CLOUDQUERY_API_KEY') }}"
        commands:
          - cloudquery sync config.yml --log-console --log-level=debug
        outputFiles:
          - "**/*.csv"

```

The flow will extract all tables from the HackerNews API and load them to CSV file. You may notice that the input to the `config_files` task is virtually identical to your CloudQuery configuration file.

import { Callout } from 'nextra-theme-docs';

<Callout type="info">
Kestra flows are built using a declarative YAML syntax, in the same way as you know it from CloudQuery.
</Callout>

Once you configured the flow in the editor, click on `Save`.

![Kestra Flow Editor Screenshot](/images/docs/deployment/kestra-flow.png)

## Step 3: Run the flow

To manually start a Kestra flow, click on the `Execute` button in the top right corner. Then, confirm by clicking on `Execute`.

You should now see the sync running in the `Executions` tab. You can navigate to the Logs tab to check the logs and validate everything runs as expected. Also, you can download the extracted CSV files from the `Outputs` tab:

![Kestra Flow Execution Screenshot](/images/docs/deployment/kestra-execution.png)

## Step 4: Schedule the flow

To run the flow periodically, we can add a trigger to run it on a schedule. Back in the Flow editor, add the following section:

```yaml copy
triggers:
  - id: schedule
    type: io.kestra.core.models.triggers.types.Schedule
    cron: "0 9 * * *" # every day at 9am
    timezone: US/Eastern
```

This cron expression will run the flow every day at 09:00. You can use [crontab.guru](https://crontab.guru/) to generate cron expressions and replace the one in the example above. Kestra also supports these special values for `cron`:

```text
@yearly
@annually
@monthly
@weekly
@daily
@midnight
@hourly
```

With this in place, remember to click `Save` again. Your CloudQuery sync will now run on a regular schedule.

## Next steps

### Caching

To speed up your syncs and save bandwidth, you can cache the downloaded plugins locally. Kestra does not provide cache for container-based tasks, but you can use a shared volume to cache the plugins.

To do this, you need to configure Kestra to [enable volume mounts](https://kestra.io/docs/configuration-guide/tasks#kestratasksscriptsdockervolume-enabled) first.

Then configure your CloudQuery task to use the shared volume:

```yaml
- id: run_sync
  type: io.kestra.plugin.cloudquery.CloudQueryCLI
  env:
    CLOUDQUERY_API_KEY: "{{ secret('CLOUDQUERY_API_KEY') }}"
  # Mount the shared volume to the CloudQuery container
  docker:
    image: ghcr.io/cloudquery/cloudquery:latest
    volumes:
      - "/tmp/cq:/app/.cq"
  commands:
  # Use the shared volume for downloading plugins
    - cloudquery sync config.yml --log-console --log-level=debug  --cq-dir=/app/.cq
```

Note: The shared volume is mounted to the `/tmp/cq` directory in the example above on the host machine. You can adjust the path to your needs.

### Production deployment

This tutorial was just a quick introduction to help you get started with a CloudQuery deployment on [Kestra](https://kestra.io/). You can now create additional Kestra tasks to perform transformations, send notifications and more. For more information, check out the [CloudQuery docs](/docs) and the [Kestra docs](https://kestra.io/docs/).

To productionize your Kestra deployment, you will likely need to deploy it to a cloud environment, such as Kubernetes. For more information, see the [Kestra Deployment with Kubernetes guide](https://kestra.io/docs/installation/kubernetes) and if you have any questions, you can reach the Kestra team [via Community Slack](https://kestra.io/slack).
