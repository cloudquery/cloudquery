---
name: S3
stage: GA
title: S3 Destination Plugin
description: CloudQuery S3 destination plugin documentation
---
# S3 Destination Plugin

import { getLatestVersion } from "../../../../../utils/versions";
import { Badge } from "../../../../../components/Badge";
import Configuration from "./_configuration.mdx";
import Authentication from "./_authentication.mdx";

<Badge text={"Latest: " + getLatestVersion("destination", "s3")}/>

This destination plugin lets you sync data from a CloudQuery source to remote S3 storage in various formats such as CSV, JSON and Parquet.

This is useful in various use-cases, especially in data lakes where you can query the data direct from Athena or load it to various data warehouses such as BigQuery, RedShift, Snowflake and others.

## Example

<Configuration />

The S3 destination utilizes batching, and supports `batch_size`, `batch_size_bytes` and `batch_timeout` options (see below).

## S3 Spec

This is the (nested) spec used by the CSV destination Plugin.

- `bucket` (`string`) (required)

  Bucket where to sync the files.

- `region` (`string`) (required)

  Region where bucket is located.

- `path` (`string`) (required)

  Path to where the files will be uploaded in the above bucket. The path supports the following placeholder variables:

  - `{{TABLE}}` will be replaced with the table name
  - `{{FORMAT}}` will be replaced with the file format, such as `csv`, `json` or `parquet`. If compression is enabled, the format will be `csv.gz`, `json.gz` etc.
  - `{{UUID}}` will be replaced with a random UUID to uniquely identify each file
  - `{{YEAR}}` will be replaced with the current year in `YYYY` format
  - `{{MONTH}}` will be replaced with the current month in `MM` format
  - `{{DAY}}` will be replaced with the current day in `DD` format
  - `{{HOUR}}` will be replaced with the current hour in `HH` format
  - `{{MINUTE}}` will be replaced with the current minute in `mm` format

  Note that timestamps are in UTC and will be the current time at the time the file is written, not when the sync started.

- `format` (string) (required)

  Format of the output file. Supported values are `csv`, `json` and `parquet`.

- `format_spec` ([format_spec](#format_spec)) (optional)

  Optional parameters to change the format of the file.

- `compression` (`string`) (optional) (default: empty)

  Compression algorithm to use. Supported values are empty or `gzip`. Not supported for `parquet` format.

- `no_rotate` (`boolean`) (optional) (default: `false`)

  If set to `true`, the plugin will write to one file per table.
  Otherwise, for every batch a new file will be created with a different `.<UUID>` suffix.

- `athena` (`boolean`) (optional) (default: `false`)

  When `athena` is set to `true`, the S3 plugin will sanitize keys in JSON columns to be compatible with the Hive Metastore / Athena.
  This allows tables to be created with a Glue Crawler and then queried via Athena, without changes to the table schema.

- `test_write` (`boolean`) (optional) (default: `true`)

  Ensure write access to the given bucket and path by writing a test object on each sync.
  If you are sure that the bucket and path are writable, you can set this to `false` to skip the test.

- `endpoint` (`string`) (optional) (default: empty)

  Endpoint to use for S3 API calls. This is useful for S3-compatible storage services such as MinIO.
  Note: if you want to use path-style addressing, i.e., `https://s3.amazonaws.com/BUCKET/KEY`, `use_path_style` should be enabled, too.

- `use_path_style` (`boolean`) (optional) (default: `false`)

  Allows to use path-style addressing in the `endpoint` option, i.e., `https://s3.amazonaws.com/BUCKET/KEY`.
  By default, the S3 client will use virtual hosted bucket addressing when possible (`https://BUCKET.s3.amazonaws.com/KEY`).

- `batch_size` (`integer`) (optional) (default: `10000`)

  Number of records to write before starting a new object.

- `batch_size_bytes` (`integer`) (optional) (default: `52428800` (= 50 MiB))

  Number of bytes (as Arrow buffer size) to write before starting a new object.

- `batch_timeout` (`duration`) (optional) (default: `30s` (30 seconds))

  Inactivity time before starting a new object.

### format_spec

- `delimiter` (`string`) (optional) (default: `,`)

  Character that will be used as want to use as the delimiter if the format type is `csv`

- `skip_header` (`boolean`) (optional) (default: `false`)

  Specifies if the first line of a file should be the headers (when format is `csv`).

## Authentication

<Authentication />
