---
name: Snowflake
title: Snowflake Destination Plugin
description: CloudQuery Snowflake destination plugin documentation
---
# Snowflake Destination Plugin

import { getLatestVersion } from "../../../../../utils/versions";
import { Badge } from "../../../../../components/Badge";
import Configuration from "./_configuration.mdx";

<Badge text={"Latest: " + getLatestVersion("destination", "snowflake")}/>

The snowflake plugin helps you sync data to your Snowflake data warehouse.

There are two ways to sync data to Snowflake:

1. Direct (easy but not recommended for production or large data sets): This is the default mode of operation where CQ plugin will stream the results directly to the Snowflake database. There is no additional setup needed apart from authentication to Snowflake.

2. Loading via CSV/JSON from a remote storage: This is the standard way of loading data into Snowflake, it is recommended for production and large data sets. This mode requires a remote storage (e.g. S3, GCS, Azure Blob Storage) and a Snowflake stage to be created. The CQ plugin will stream the results to the remote storage. You can then load those files via a cronjob or via SnowPipe. This method is still in the works and will be updated soon with a guide.

## Example Config

<Configuration />

The Snowflake destination utilizes batching, and supports [`batch_size`](/docs/reference/destination-spec#batch_size) and [`batch_size_bytes`](/docs/reference/destination-spec#batch_size_bytes).

## Snowflake Spec

This is the top level spec used by the Snowflake destination plugin.

- `connection_string` (`string`) (required)

  Snowflake `connection_string`.

  Example:

  ```yaml copy
  # user[:password]@account/database/schema?warehouse=user_warehouse[&param1=value1&paramN=valueN]
  # or
  # user[:password]@account/database?warehouse=user_warehouse[&param1=value1&paramN=valueN]
  # or
  # user[:password]@host:port/database/schema?account=user_account&warehouse=user_warehouse[&param1=value1&paramN=valueN]
  # or
  # host:port/database/schema?account=user_account&warehouse=user_warehouse[&param1=value1&paramN=valueN]
  ```

  From Snowflake documentation:

  `account` - Name assigned to your Snowflake account. If you are not on us-west-2 or AWS deployment, append the region and platform to the end, e.g., `<account>.<region> or <account>.<region>.<platform>`.


- `migrate_concurrency` (`integer`) (optional) (default: `1`)

  By default, tables are migrated one at a time.
  This option allows you to migrate multiple tables concurrently.
  This can be useful if you have a lot of tables to migrate and want to speed up the process.

  Setting this to a negative number means no limit.

- `batch_size` (`integer`) (optional) (default: `1000`)

  Number of records to batch together before sending to the database.

- `batch_size_bytes` (`integer`) (optional) (default: `4194304` (= 4 MiB))

  Number of bytes (as Arrow buffer size) to batch together before sending to the database.

## Underlying library

We use the official [github.com/snowflakedb/gosnowflake](https://github.com/snowflakedb/gosnowflake) package for database connection.
